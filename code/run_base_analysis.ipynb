{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdd09e22",
   "metadata": {},
   "source": [
    "# Non Randomness in Dinamic Social Networks\n",
    "\n",
    "## Research Questions\n",
    "1. Equilibrium dynamics: Are transition probabilities stationary and is detailed balance approximately satisfied over time?\n",
    "2. Hub persistence: Are there any overlapping hubs that retain structural roles across snapshots, and how does burstiness affect hub collapse?\n",
    "3. Predictive stability: Can motif, sentiment, and activity features predict community persistence?\n",
    "4. Sentiment coevolution: Does sentiment polarization (VADER) co-evolve with community structure and indicate stability?\n",
    "\n",
    "\n",
    "## Dataset metadata (from the source website)\n",
    "\n",
    "* **Scope:** Timestamps from their creation up to **October 2018**.\n",
    "* **Structure:**\n",
    "  * Each **subreddit** has its own corpus (`subreddit-[name]`).\n",
    "  * Each **post + comment thread** is a *conversation*.\n",
    "  * Each **comment or post** is an *utterance*.\n",
    "  * Each **user** is a *speaker* (identified by Reddit username).\n",
    "* **Utterance-level fields:** ID, speaker, conversation ID, reply-to ID, timestamp, text, score, gilding info, stickied status, permalink, and author flair.\n",
    "* **Conversation-level fields:** Title, number of comments, domain, subreddit name, gilding, stickied status, and author flair.\n",
    "* **Corpus-level fields:** Subreddit name, total posts, total comments, and number of unique speakers.\n",
    "* **Usage:** Can be downloaded as a zip file or loaded with `convokit`, and combined with other subreddit corpora for cross-community analysis.\n",
    "* **Notes:**\n",
    "  * Some subreddit data may be incomplete or contain broken thread links.\n",
    "  * Large subreddits may have very large corpus files.\n",
    "  * Speaker counts may be inflated due to duplicates in preprocessing.\n",
    "  * Dataset is **beta** and subject to updates for completeness and data consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2045ce49",
   "metadata": {},
   "source": [
    "# Run the full analysis pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5efa1203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REDDIT USER-USER INTERACTIONS NETWORK ANALYSIS\n",
      "======================================================================\n",
      "CONFIGURATION:\n",
      "    • Snapshots per year: 4\n",
      "    • Minimum GCC size: 10000 nodes\n",
      "    • Overlapping community detection enabled: No (Louvain)\n",
      "    • Checkpoints enabled: Yes (../data/02_preprocessed)\n",
      "    • Parallel workers: 11\n",
      "Initialized Reddit Network Analyzer with 11 parallel workers\n",
      "Loading raw data...\n",
      "Attempting to reuse checkpoint...\n",
      "Loaded checkpoint: full_data_raw.pkl\n",
      "Using cached data\n",
      "Data loading complete\n",
      "Created global comment user map with 2,598,721 entries\n",
      "Loaded:\n",
      "    • Posts: 194,036\n",
      "    • Comments: 2,598,721\n",
      "    • Non-empty comments: 2,247,049\n",
      "    • Unique users: 487,982\n",
      "Sample of comments:\n",
      "Comment 0 (780 characters): Hey,\n",
      "I've been looking for a good documentary film podcast but haven't found any. Can anyone recomme...\n",
      "Comment 1 (119 characters): I would love to get my hands on a copy but wanted to know if it was any good and/or if anybody knew ...\n",
      "Comment 2 (9 characters): [removed]...\n",
      "Comment 3 (196 characters): An awesome documentary filmed on the ground in Haiti just two weeks after the earthquake. Check it o...\n",
      "Comment 4 (198 characters): I am unable to find it on DVD and it's nowhere to be found on the internets. Anybody have any leads?...\n",
      "Preprocessing comment data...\n",
      "Attempting to reuse checkpoint...\n",
      "Loaded checkpoint: full_data_preprocessed.pkl\n",
      "Using cached data\n",
      "Preprocessed:\n",
      "    • Comments: 2,138,495\n",
      "Sample of comments:\n",
      "Comment 0 (1302 characters): This documentary covers the fourth game of Humans vs. Zombies, a modified tag game that originated a...\n",
      "Comment 1 (82 characters): Will Film Director Roman Polanski be alllowed to return to the United States soon?...\n",
      "Comment 2 (134 characters): is this the show where they measure how much of the human body is made up of corn carbon? and the ch...\n",
      "Comment 3 (1385 characters): Well, they start out by saying that this is the first generation that will have a shorter lifespan t...\n",
      "Comment 4 (67 characters): But it's cheap, fast, and easy! Anything else would be un-American....\n",
      "Starting parallel network construction...\n",
      "Analyzing network giant component sizes across all requested snapshots...\n",
      "    Scanning 42 periods in parallel...\n",
      "    Progress: 4/42 periods analyzed...\n",
      "    Progress: 8/42 periods analyzed...\n",
      "    Progress: 12/42 periods analyzed...\n",
      "    Progress: 16/42 periods analyzed...\n",
      "    Progress: 20/42 periods analyzed...\n",
      "    Progress: 24/42 periods analyzed...\n",
      "    Progress: 28/42 periods analyzed...\n",
      "    Progress: 32/42 periods analyzed...\n",
      "    Progress: 36/42 periods analyzed...\n",
      "    Progress: 40/42 periods analyzed...\n",
      "    Progress: 42/42 periods analyzed...\n",
      "Giant component size analysis complete!\n",
      "Summary:\n",
      "    • Periods analyzed: 42\n",
      "    • Periods meeting minimum GCC node count (≥ 10,000: 15\n",
      "    • Largest GCC: 3 nodes in period 2008#7\n",
      "    • Average GCC size: 7,157 nodes\n",
      "Creating snapshots of the network for the selected periods...\n",
      "    Built networks for 1/15 selected periods...\n",
      "    Built networks for 2/15 selected periods...\n",
      "    Built networks for 3/15 selected periods...\n",
      "    Built networks for 4/15 selected periods...\n",
      "    Built networks for 5/15 selected periods...\n",
      "    Built networks for 6/15 selected periods...\n",
      "    Built networks for 7/15 selected periods...\n",
      "    Built networks for 8/15 selected periods...\n",
      "    Built networks for 9/15 selected periods...\n",
      "    Built networks for 10/15 selected periods...\n",
      "    Built networks for 11/15 selected periods...\n",
      "    Built networks for 12/15 selected periods...\n",
      "    Built networks for 13/15 selected periods...\n",
      "    Built networks for 14/15 selected periods...\n",
      "    Built networks for 15/15 selected periods...\n",
      "Checkpoint saved: user_network_month_2017#1.pkl\n",
      "Checkpoint saved: user_network_month_2017#10.pkl\n",
      "Checkpoint saved: user_network_month_2017#7.pkl\n",
      "Checkpoint saved: user_network_month_2017#4.pkl\n",
      "Checkpoint saved: user_network_month_2018#1.pkl\n",
      "Checkpoint saved: user_network_month_2016#10.pkl\n",
      "Checkpoint saved: user_network_month_2018#7.pkl\n",
      "Checkpoint saved: user_network_month_2018#4.pkl\n",
      "Checkpoint saved: user_network_month_2016#1.pkl\n",
      "Checkpoint saved: user_network_month_2016#7.pkl\n",
      "Checkpoint saved: user_network_month_2018#10.pkl\n",
      "Checkpoint saved: user_network_month_2016#4.pkl\n",
      "Checkpoint saved: user_network_month_2015#10.pkl\n",
      "Checkpoint saved: user_network_month_2015#7.pkl\n",
      "Checkpoint saved: user_network_month_2015#4.pkl\n",
      "======================================================================\n",
      "COMMUNITY DETECTION\n",
      "======================================================================\n",
      "Detection mode: Non-overlapping communities (Louvain)\n",
      "Target: Observed network\n",
      "Loaded checkpoint: community_memberships.pkl\n",
      "Using checkpoint data for network 2017#1. Skipping community detection!\n",
      "Using checkpoint data for network 2017#10. Skipping community detection!\n",
      "Using checkpoint data for network 2017#7. Skipping community detection!\n",
      "Using checkpoint data for network 2017#4. Skipping community detection!\n",
      "Using checkpoint data for network 2018#1. Skipping community detection!\n",
      "Using checkpoint data for network 2016#10. Skipping community detection!\n",
      "Using checkpoint data for network 2018#7. Skipping community detection!\n",
      "Using checkpoint data for network 2018#4. Skipping community detection!\n",
      "Using checkpoint data for network 2016#1. Skipping community detection!\n",
      "Using checkpoint data for network 2016#7. Skipping community detection!\n",
      "Using checkpoint data for network 2018#10. Skipping community detection!\n",
      "Using checkpoint data for network 2016#4. Skipping community detection!\n",
      "Using checkpoint data for network 2015#10. Skipping community detection!\n",
      "Using checkpoint data for network 2015#7. Skipping community detection!\n",
      "Using checkpoint data for network 2015#4. Skipping community detection!\n",
      "Checkpoint saved: community_memberships.pkl\n",
      "Calculating relevant metrics...\n",
      "Loaded checkpoint: metrics.pkl\n",
      "    Processing period 2017#1...\n",
      "    Processing period 2017#10...\n",
      "    Processing period 2017#7...\n",
      "    Processing period 2017#4...\n",
      "    Processing period 2018#1...\n",
      "    Processing period 2016#10...\n",
      "    Processing period 2018#7...\n",
      "    Processing period 2018#4...\n",
      "    Processing period 2016#1...\n",
      "    Processing period 2016#7...\n",
      "    Processing period 2018#10...\n",
      "    Processing period 2016#4...\n",
      "    Processing period 2015#10...\n",
      "    Processing period 2015#7...\n",
      "    Processing period 2015#4...\n",
      "Exported numeric metrics to ../data/04_reports/metrics.csv\n",
      "======================================================================\n",
      "ANALYSIS COMPLETED SUCCESSFULLY\n",
      "======================================================================\n",
      "Processed data: ../data/02_preprocessed\n",
      "Reports: ../data/04_reports\n"
     ]
    }
   ],
   "source": [
    "import src.pipeline as pipeline\n",
    "\n",
    "analyzer = pipeline.run_analysis(\n",
    "    data_path=\"../data/01_raw\",\n",
    "    snapshots_per_year=4,\n",
    "    min_giant_component_size=10_000,\n",
    "    overlapping_communities=False,  # Set to False for non-overlapping (Louvain) communities\n",
    "    use_checkpoints=True,\n",
    "    checkpoint_dir=\"../data/02_preprocessed\",\n",
    "    reports_dir=\"../data/04_reports\",\n",
    "    n_workers=11,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv_ml_venv_sys (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
